
# -*- coding: utf-8 -*-
"""ensembling.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/104gkFTtA5CbJqxBcpgKgn46PpRGovLmn
"""

import sys
sys.path.append('/content/drive/MyDrive/repos/image_processing_pipeline')


import tensorflow as tf
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger, TensorBoard
from tensorflow.keras.optimizers import Adam
from dataclasses import dataclass
from runner import *
from trainer import *
from initial_parser import *
from splitter import *
from data_provider import *
from custom_models import *
from custom_metrics import *
from custom_losses import *
from iterstrat.ml_stratifiers import MultilabelStratifiedKFold
from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit
import os
import random
from tensorflow.keras.layers.experimental import preprocessing 
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import albumentations as A
import numpy as np
from keras.models import Model
from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D
from keras.layers import BatchNormalization
from keras.layers.core import SpatialDropout2D, Activation
from tensorflow.keras.applications import EfficientNetB0
from keras import backend as K
from keras.layers.merge import concatenate
from keras.utils.data_utils import get_file
from tensorflow import keras
import segmentation_models as sm
import cv2
import datetime
from segmentation_models.losses import bce_jaccard_loss
import pickle
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.ensemble import GradientBoostingClassifier

class ensemble_utils():
  def __init__(self, parser, img_size, nn_zoo):
      self.parser = parser
      self.img_size = img_size
      self.nn_zoo = nn_zoo

  def nn_ensemble_prediction(self, input_image):
    all_predictions = list()
    for nn in self.nn_zoo:
      prediction = nn.predict(input_image)
      all_predictions.append(prediction)
    return tf.concat(all_predictions, axis = -1)   


  def prepare_data_for_second_layer_model(self, second_layer_train_ds):
    X_data = list()
    Y_data = list()
    for item in second_layer_train_ds:
      mask = self.parser.parse_mask(item)
      mask = tf.reshape(mask, [self.img_size*self.img_size,1])
      Y_data.append(mask)
      image = self.parser.parse_image(item)
      image = tf.expand_dims(image, axis = 0)
      multiple_pred = self.nn_ensemble_prediction(image)
      multiple_pred = tf.reshape(multiple_pred, [self.img_size*self.img_size, len(self.nn_zoo)])
      X_data.append(multiple_pred)
    return tf.concat(X_data, axis = 0), tf.concat(Y_data, axis = 0)
   

class ensemble_trainer():
  def __init__(self, parser, img_size, nn_zoo):
      self.parser = parser
      self.img_size = img_size
      self.nn_zoo = nn_zoo
      self.ensemble_utils_obj = ensemble_utils(self.parser, self.img_size, self.nn_zoo)

  def train(self, second_layer_model_obj, second_layer_model_path, second_layer_train_ds):
    inputX,inputY=self.ensemble_utils_obj.prepare_data_for_second_layer_model(second_layer_train_ds)
    fitted_model = second_layer_model_obj.fit(inputX.numpy(), inputY.numpy())
    pickle.dump(fitted_model, open(second_layer_model_path, "wb"))




class ensemble_predictor():
  def __init__(self, parser, img_size, nn_zoo, fitted_second_layer_model):
      self.parser = parser
      self.img_size = img_size
      self.nn_zoo = nn_zoo
      self.fitted_second_layer_model = fitted_second_layer_model
      self.ensemble_utils_obj = ensemble_utils(self.parser, self.img_size, self.nn_zoo)

  def predict(self, prepared_image):
    multiple_pred = self.ensemble_utils_obj.nn_ensemble_prediction(prepared_image)
    multiple_pred = tf.reshape(multiple_pred, [self.img_size*self.img_size, len(self.nn_zoo)])
    result = self.fitted_second_layer_model.predict(multiple_pred.numpy())
    result = np.reshape(result,[1,self.img_size,self.img_size,1])
    return result


